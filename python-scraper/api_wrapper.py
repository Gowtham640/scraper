#!/usr/bin/env python3
"""
API Wrapper for SRM Academia Portal Scraper
Provides clean functions for Next.js to call
Based on the working standalone code with smart caching
"""

import sys
import json
import time
import os
from datetime import datetime, timedelta
from scraper_selenium_session import SRMAcademiaScraperSelenium
from calendar_scraper_fixed import extract_calendar_data_from_html
from timetable_scraper import api_get_timetable_data, get_timetable_page_html, extract_timetable_data_from_html, create_do_timetable_json
import re
from bs4 import BeautifulSoup

# ============================================================================
# CACHING CONFIGURATION
# ============================================================================

CACHE_FILE = "calendar_cache.json"
CACHE_DURATION_HOURS = 6  # Cache for 6 hours


def is_cache_valid():
    """Check if cached calendar data is still valid"""
    if not os.path.exists(CACHE_FILE):
        print("[CACHE] No cache file found", file=sys.stderr)
        return False

    try:
        with open(CACHE_FILE, 'r') as f:
            cache_data = json.load(f)

        cache_time_str = cache_data.get('timestamp', '')
        if not cache_time_str:
            print("[CACHE] No timestamp in cache", file=sys.stderr)
            return False

        cache_time = datetime.fromisoformat(cache_time_str)
        if datetime.now() - cache_time > timedelta(hours=CACHE_DURATION_HOURS):
            print("[CACHE] Cache expired", file=sys.stderr)
            return False

        print("[CACHE] Cache is valid", file=sys.stderr)
        return True
    except Exception as e:
        print(f"[CACHE] Error reading cache: {e}", file=sys.stderr)
        return False


def get_cached_calendar_data():
    """Get calendar data from cache"""
    try:
        with open(CACHE_FILE, 'r') as f:
            cache_data = json.load(f)
        data = cache_data.get('data', [])
        print(f"[CACHE] Retrieved {len(data)} entries from cache", file=sys.stderr)
        return data
    except Exception as e:
        print(f"[CACHE] Error reading cached data: {e}", file=sys.stderr)
        return []


def save_calendar_cache(calendar_data):
    """Save calendar data to cache"""
    try:
        cache_data = {
            'data': calendar_data,
            'timestamp': datetime.now().isoformat(),
            'count': len(calendar_data),
            'cache_duration_hours': CACHE_DURATION_HOURS
        }
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache_data, f, indent=2)
        print(f"[CACHE] Saved {len(calendar_data)} entries to cache", file=sys.stderr)
    except Exception as e:
        print(f"[CACHE] Error saving cache: {e}", file=sys.stderr)


# ============================================================================
# API FUNCTIONS FOR NEXT.JS INTEGRATION
# ============================================================================

def api_get_calendar_data(email, password, force_refresh=False):
    """API function to get calendar data with smart caching"""
    scraper = None
    try:
        print(f"[API] Getting calendar data for: {email}", file=sys.stderr)

        # Check cache first (unless force refresh)
        if not force_refresh and is_cache_valid():
            cached_data = get_cached_calendar_data()
            if cached_data:
                print(f"[CACHE] Using cached data ({len(cached_data)} entries)", file=sys.stderr)
                return {
                    "success": True,
                    "data": cached_data,
                    "type": "calendar",
                    "count": len(cached_data),
                    "cached": True,
                    "cache_timestamp": datetime.now().isoformat()
                }

        print("[CACHE] Cache expired or empty - fetching fresh data", file=sys.stderr)

        # Initialize scraper with session management
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True)

        html_content = None
        # Try to get data with existing session first
        if scraper.is_session_valid():
            print("[API] Valid session found - trying to get data without login", file=sys.stderr)
            html_content = scraper.get_calendar_data()

        # If session was invalid or data fetch failed, attempt login
        if html_content is None:
            print("[API] Session invalid or expired - attempting login", file=sys.stderr)
            if not scraper.login(email, password):
                print("[API] Login failed!", file=sys.stderr)
                return {"success": False, "error": "Login failed"}
            print("[API] Login successful!", file=sys.stderr)
            html_content = scraper.get_calendar_data()

        if not html_content:
            print("[API] Failed to get calendar HTML content after all attempts", file=sys.stderr)
            return {"success": False, "error": "Failed to get calendar data"}

        print(f"[API] Got HTML content ({len(html_content)} characters)", file=sys.stderr)
        calendar_data = extract_calendar_data_from_html(html_content)

        if calendar_data:
            print(f"[API] Successfully extracted {len(calendar_data)} calendar entries", file=sys.stderr)
            save_calendar_cache(calendar_data)
            return {
                "success": True,
                "data": calendar_data,
                "type": "calendar",
                "count": len(calendar_data),
                "cached": False,
                "fresh_data": True
            }
        else:
            print("[API] No calendar data extracted", file=sys.stderr)
            return {
                "success": True,
                "data": [],
                "type": "calendar",
                "count": 0,
                "cached": False
            }
        
    except Exception as e:
        print(f"[API] Error getting calendar data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)

        if not force_refresh:
            cached_data = get_cached_calendar_data()
            if cached_data:
                print("[CACHE] Scraping failed, using stale cache as fallback", file=sys.stderr)
                return {
                    "success": True,
                    "data": cached_data,
                    "type": "calendar",
                    "count": len(cached_data),
                    "cached": True,
                    "stale": True,
                    "fallback": True
                }
        return {"success": False, "error": f"API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[API] Scraper closed", file=sys.stderr)
            except Exception as e:
                print(f"[API] Error closing scraper: {e}", file=sys.stderr)


def get_attendance_page_html(scraper):
    """Get the HTML content of the attendance page"""
    try:
        print("\n=== NAVIGATING TO ATTENDANCE PAGE ===", file=sys.stderr)
        
        # Navigate to the attendance page
        attendance_url = "https://academia.srmist.edu.in/#Page:My_Attendance"
        print(f"[STEP 1] Navigating to: {attendance_url}", file=sys.stderr)
        
        scraper.driver.get(attendance_url)
        time.sleep(0.5)  # Reduced from 2s to 0.5s - just wait for basic page structure
        
        print(f"[OK] Current URL: {scraper.driver.current_url}", file=sys.stderr)
        print(f"[OK] Page title: {scraper.driver.title}", file=sys.stderr)
        
        # Skip document.readyState wait - we disabled images, so basic structure loads quickly
        print("[OK] Attendance page loaded successfully", file=sys.stderr)
        
        # Get page source
        page_source = scraper.driver.page_source
        
        return page_source
        
    except Exception as e:
        print(f"[FAIL] Error getting attendance page: {e}", file=sys.stderr)
        return None


def extract_attendance_data_from_html(html_content):
    """
    Extract attendance data from HTML content using BeautifulSoup.
    Based on the HTML structure: table with tr rows containing subject data.
    """
    attendance_data = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Check if page shows access denied
        page_text = soup.get_text().lower()
        if 'not accessible' in page_text or 'not allowed to access' in page_text:
            print("[ERROR] Access denied to attendance page", file=sys.stderr)
            return attendance_data
        
        # Find all tables
        tables = soup.find_all('table')
        print(f"Found {len(tables)} tables on the page", file=sys.stderr)
        
        # Also look for divs that might contain table-like data
        divs_with_tables = soup.find_all('div', class_=lambda x: x and 'table' in x.lower())
        print(f"Found {len(divs_with_tables)} divs with table classes", file=sys.stderr)
        
        # Look for the main attendance table
        main_table = None
        
        # First, try to find table with attendance-related content
        for i, table in enumerate(tables):
            table_text = table.get_text().lower()
            if any(keyword in table_text for keyword in ['attendance', 'hours conducted', 'absent', 'theory', 'lab', 'subject', 'course']):
                print(f"Found potential attendance table {i}", file=sys.stderr)
                main_table = table
                break
        
        # If no table found, look for divs that might contain table data
        if not main_table:
            for i, div in enumerate(divs_with_tables):
                div_text = div.get_text().lower()
                if any(keyword in div_text for keyword in ['attendance', 'hours conducted', 'absent', 'theory', 'lab']):
                    print(f"Found potential attendance div {i}", file=sys.stderr)
                    # Look for table inside this div
                    inner_table = div.find('table')
                    if inner_table:
                        main_table = inner_table
                        break
        
        # If still no table found, try to find any table with multiple rows
        if not main_table:
            print("No attendance-specific table found, trying any table with data", file=sys.stderr)
            for table in tables:
                rows = table.find_all('tr')
                if len(rows) > 1:  # More than just header
                    print(f"Found table with {len(rows)} rows", file=sys.stderr)
                    main_table = table
                    break
        
        if not main_table:
            print("No suitable table found", file=sys.stderr)
            return attendance_data
        
        print("Processing attendance table...", file=sys.stderr)
        
        # Get all rows from the table
        rows = main_table.find_all('tr')
        print(f"Found {len(rows)} rows in attendance table", file=sys.stderr)
        
        # Process each row (skip first empty row if it exists)
        for i, row in enumerate(rows):
            cells = row.find_all('td')
            
            # Skip rows with insufficient cells or empty rows
            if len(cells) < 9:
                print(f"Skipping row {i}: insufficient cells ({len(cells)})", file=sys.stderr)
                continue
            
            # Check if this is a data row (not header)
            row_text = row.get_text(strip=True)
            if not row_text or len(row_text) < 10:  # Skip empty or very short rows
                print(f"Skipping row {i}: empty or too short", file=sys.stderr)
                continue
            
            try:
                # Extract data from each cell
                subject_code = cells[0].get_text(strip=True)
                course_title = cells[1].get_text(strip=True)
                category = cells[2].get_text(strip=True)
                faculty_name = cells[3].get_text(strip=True)
                slot = cells[4].get_text(strip=True) if len(cells) > 4 else ""
                room = cells[5].get_text(strip=True) if len(cells) > 5 else ""
                hours_conducted = cells[6].get_text(strip=True)
                hours_absent = cells[7].get_text(strip=True)
                attendance = cells[8].get_text(strip=True)
                
                # Validate that we have meaningful data
                if course_title and course_title != "Course Title":  # Skip header row
                    attendance_entry = {
                        'row_number': i,
                        'subject_code': subject_code,
                        'course_title': course_title,
                        'category': category,
                        'faculty_name': faculty_name,
                        'slot': slot,
                        'room': room,
                        'hours_conducted': hours_conducted,
                        'hours_absent': hours_absent,
                        'attendance': attendance,
                        'attendance_percentage': calculate_attendance_percentage(hours_conducted, hours_absent)
                    }
                    
                    attendance_data.append(attendance_entry)
                    print(f"[DATA] Row {i}: {course_title} - {attendance}% attendance", file=sys.stderr)
                
            except Exception as e:
                print(f"[WARN] Error processing row {i}: {e}", file=sys.stderr)
                continue
        
        print(f"Extracted {len(attendance_data)} attendance entries", file=sys.stderr)
    
    except Exception as e:
        print(f"Error extracting attendance data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
    
    return attendance_data


def calculate_attendance_percentage(hours_conducted, hours_absent):
    """Calculate attendance percentage"""
    try:
        conducted = int(hours_conducted) if hours_conducted.isdigit() else 0
        absent = int(hours_absent) if hours_absent.isdigit() else 0
        
        if conducted == 0:
            return "0%"
        
        attended = conducted - absent
        percentage = (attended / conducted) * 100
        return f"{percentage:.1f}%"
    
    except Exception:
        return "N/A"


def extract_semester_from_html(html_content):
    """
    Extract semester information from attendance page HTML
    Looks specifically in the second table for semester information
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find all tables on the page
        tables = soup.find_all('table')
        print(f"[SEMESTER] Found {len(tables)} tables on the page", file=sys.stderr)
        
        # Focus on the second table (index 1) as the user specified
        if len(tables) >= 2:
            second_table = tables[1]
            print("[SEMESTER] ✓ Targeting second table (Student Info)", file=sys.stderr)
            
            # Get all rows in the second table
            rows = second_table.find_all('tr')
            print(f"[SEMESTER] Found {len(rows)} rows in second table", file=sys.stderr)
            
            # Look through each row for "Semester:"
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                
                # Check each cell in the row
                for j, cell in enumerate(cells):
                    cell_text = cell.get_text(strip=True)
                    
                    # If this cell contains "Semester:"
                    if 'semester' in cell_text.lower() and ':' in cell_text:
                        print(f"[SEMESTER] Found 'Semester:' in row {i}, cell {j}: '{cell_text}'", file=sys.stderr)
                        
                        # The semester NUMBER is in the NEXT cell (j+1)
                        if j + 1 < len(cells):
                            next_cell = cells[j + 1]
                            semester_text = next_cell.get_text(strip=True)
                            
                            print(f"[SEMESTER] Next cell content: '{semester_text}'", file=sys.stderr)
                            
                            # Extract number from the cell (handle <strong>3</strong> or just "3")
                            semester_match = re.search(r'(\d)', semester_text)
                            if semester_match:
                                semester = int(semester_match.group(1))
                                if 1 <= semester <= 8:
                                    print(f"[SEMESTER] ✓✓✓ Extracted semester: {semester}", file=sys.stderr)
                                    return semester
                        else:
                            print(f"[SEMESTER] No next cell found (j={j}, total cells={len(cells)})", file=sys.stderr)
        
        # Fallback: try the second table's text directly for semester info
        if len(tables) >= 2:
            second_table_text = tables[1].get_text().lower()
            semester_match = re.search(r'semester\s*:?\s*(\d)', second_table_text)
            if semester_match:
                semester = int(semester_match.group(1))
                if 1 <= semester <= 8:
                    print(f"[SEMESTER] Found in second table text: {semester}", file=sys.stderr)
                    return semester
        
        # Last resort: check all tables
        print("[SEMESTER] Checking all tables for semester info", file=sys.stderr)
        for i, table in enumerate(tables):
            table_text = table.get_text().lower()
            if 'semester' in table_text:
                semester_match = re.search(r'semester\s*:?\s*(\d)', table_text)
                if semester_match:
                    semester = int(semester_match.group(1))
                    if 1 <= semester <= 8:
                        print(f"[SEMESTER] Found in table {i}: {semester}", file=sys.stderr)
                        return semester
        
        print("[SEMESTER] No semester info found, defaulting to 1", file=sys.stderr)
        return 1  # Default to semester 1
        
    except Exception as e:
        print(f"[SEMESTER] Error extracting semester: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return 1  # Default to semester 1


def create_attendance_json(attendance_data, semester=1):
    """Create JSON structure with attendance data"""
    
    # Calculate summary statistics
    total_subjects = len(attendance_data)
    total_hours_conducted = 0
    total_hours_absent = 0
    
    for entry in attendance_data:
        try:
            total_hours_conducted += int(entry['hours_conducted']) if entry['hours_conducted'].isdigit() else 0
            total_hours_absent += int(entry['hours_absent']) if entry['hours_absent'].isdigit() else 0
        except:
            continue
    
    overall_attendance = calculate_attendance_percentage(str(total_hours_conducted), str(total_hours_absent))
    
    # Group by category
    theory_subjects = [entry for entry in attendance_data if entry['category'].lower() == 'theory']
    lab_subjects = [entry for entry in attendance_data if entry['category'].lower() == 'lab']
    other_subjects = [entry for entry in attendance_data if entry['category'].lower() not in ['theory', 'lab']]
    
    # Create the complete JSON structure
    attendance_json = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source": "SRM Academia Portal",
            "academic_year": "2025-26 ODD",
            "semester": semester,
            "institution": "SRM Institute of Science and Technology",
            "college": "College of Engineering and Technology",
            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        },
        "summary": {
            "total_subjects": total_subjects,
            "theory_subjects": len(theory_subjects),
            "lab_subjects": len(lab_subjects),
            "other_subjects": len(other_subjects),
            "total_hours_conducted": total_hours_conducted,
            "total_hours_absent": total_hours_absent,
            "overall_attendance_percentage": overall_attendance
        },
        "subjects": {
            "theory": theory_subjects,
            "lab": lab_subjects,
            "other": other_subjects
        },
        "all_subjects": attendance_data
    }
    
    return attendance_json


def api_get_attendance_data(email, password):
    """API function to get attendance data with session management"""
    scraper = None
    try:
        print(f"[API] Getting attendance data for: {email}", file=sys.stderr)
        
        # Initialize scraper with session management
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True)
        
        html_content = None
        # Try to get data with existing session first
        if scraper.is_session_valid():
            print("[API] Valid session found - trying to get data without login", file=sys.stderr)
            html_content = get_attendance_page_html(scraper)
        
        # If session was invalid or data fetch failed, attempt login
        if html_content is None:
            print("[API] Session invalid or expired - attempting login", file=sys.stderr)
            if not scraper.login(email, password):
                print("[API] Login failed!", file=sys.stderr)
                return {"success": False, "error": "Login failed"}
            print("[API] Login successful!", file=sys.stderr)
            html_content = get_attendance_page_html(scraper)

        if not html_content:
            print("[API] Failed to get attendance HTML content after all attempts", file=sys.stderr)
            return {"success": False, "error": "Failed to get attendance data"}
        
        print(f"[API] Got HTML content ({len(html_content)} characters)", file=sys.stderr)
        attendance_data = extract_attendance_data_from_html(html_content)
        
        # Extract semester from HTML
        semester = extract_semester_from_html(html_content)
        print(f"[API] Extracted semester: {semester}", file=sys.stderr)
        
        if attendance_data:
            print(f"[API] Successfully extracted {len(attendance_data)} attendance entries", file=sys.stderr)
            attendance_json = create_attendance_json(attendance_data, semester)
            return {
                "success": True,
                "data": attendance_json,
                "type": "attendance",
                "count": len(attendance_data),
                "semester": semester,
                "cached": False
            }
        else:
            print("[API] No attendance data extracted", file=sys.stderr)
            return {
                "success": True,
                "data": {"all_subjects": [], "summary": {"total_subjects": 0}},
                "type": "attendance",
                "count": 0,
                "cached": False
            }
        
    except Exception as e:
        print(f"[API] Error getting attendance data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[API] Scraper closed", file=sys.stderr)
            except Exception as e:
                print(f"[API] Error closing scraper: {e}", file=sys.stderr)


def get_marks_page_html(scraper):
    """Get the HTML content of the marks page (which is actually the attendance page)"""
    try:
        print("\n=== NAVIGATING TO MARKS PAGE (ATTENDANCE PAGE) ===", file=sys.stderr)
        
        # Navigate to the attendance page (where marks data is actually located)
        attendance_url = "https://academia.srmist.edu.in/#Page:My_Attendance"
        print(f"[MARKS] Navigating to: {attendance_url}", file=sys.stderr)
        
        scraper.driver.get(attendance_url)
        time.sleep(0.5)  # Optimized - reduced from 2s to 0.5s
        
        print(f"[MARKS] Current URL: {scraper.driver.current_url}", file=sys.stderr)
        print(f"[MARKS] Page title: {scraper.driver.title}", file=sys.stderr)
        
        # ✅ CRITICAL: Check for login page IMMEDIATELY
        current_title = scraper.driver.title
        page_source_check = scraper.driver.page_source[:1000] if len(scraper.driver.page_source) > 0 else ""
        
        if "Login" in current_title or "signinFrame" in page_source_check:
            print(f"[MARKS] ✗ ERROR: Redirected to login page - session expired or login failed", file=sys.stderr)
            print(f"[MARKS] Title: {current_title}, Contains signinFrame: {'signinFrame' in page_source_check}", file=sys.stderr)
            return None
        
        # Get page source
        page_source = scraper.driver.page_source
        page_length = len(page_source)
        print(f"[MARKS] Page source length: {page_length} characters", file=sys.stderr)
        
        # ✅ CRITICAL: Double-check we're not on login page (even if page is large)
        if "Login" in scraper.driver.title or "signinFrame" in page_source[:5000]:
            print(f"[MARKS] ✗ ERROR: Login page detected despite large page size", file=sys.stderr)
            print(f"[MARKS] Title: {scraper.driver.title}", file=sys.stderr)
            return None
        
        # Check if we got valid content
        if page_source and page_length > 2000:
            print(f"[MARKS] ✓ Got attendance page with marks data", file=sys.stderr)
            return page_source
        else:
            print(f"[MARKS] ✗ Page too small or empty: {page_length} bytes", file=sys.stderr)
            return None
        
    except Exception as e:
        print(f"[MARKS] ✗ Error getting marks page: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return None


def extract_course_titles_from_html(html_content):
    """Extract course titles from the attendance table"""
    course_titles = {}
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find all tables
        tables = soup.find_all('table')
        
        # Look for the attendance table (first table with course titles)
        for table in tables:
            table_text = table.get_text()
            # Look for attendance-related keywords or course title header
            if any(keyword in table_text for keyword in ['Course Title', 'Attn %', 'Hours Conducted', 'Hours Absent', 'Faculty Name']):
                print("Found attendance table with course titles", file=sys.stderr)
                
                # Process rows in attendance table
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 3:
                        # Skip header row
                        if 'Course Code' in str(cells[0]):
                            continue
                            
                        # First cell contains course code, second cell contains course title
                        course_code_cell = cells[0]
                        course_title_cell = cells[1]
                        
                        # Extract course code
                        course_code_text = course_code_cell.get_text(strip=True)
                        course_code_match = re.search(r'(\d{2}[A-Z]{3}\d{3}[A-Z])', course_code_text)
                        
                        if course_code_match:
                            course_code = course_code_match.group(1)
                            course_title = course_title_cell.get_text(strip=True)
                            
                            # Clean up course title (remove extra whitespace, newlines)
                            course_title = ' '.join(course_title.split())
                            
                            # Skip if course title is empty or just whitespace
                            if course_title and course_title.strip():
                                # Only add if not already present (prevent overwriting correct titles)
                                if course_code not in course_titles:
                                    course_titles[course_code] = course_title
                                    print(f"[TITLE] {course_code}: {course_title}", file=sys.stderr)
                                else:
                                    print(f"[SKIP] {course_code}: Already have title '{course_titles[course_code]}', skipping '{course_title}'", file=sys.stderr)
                
                break
        
        print(f"Extracted {len(course_titles)} course titles", file=sys.stderr)
        
    except Exception as e:
        print(f"Error extracting course titles: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
    
    return course_titles


def extract_marks_data_from_html(html_content, course_titles=None):
    """
    Extract marks data from HTML content using the old working logic.
    """
    marks_data = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Check if page shows access denied
        page_text = soup.get_text().lower()
        if 'not accessible' in page_text or 'not allowed to access' in page_text:
            print("[MARKS EXTRACT] Access denied to marks page", file=sys.stderr)
            return marks_data
        
        print(f"[MARKS EXTRACT] Starting extraction from {len(html_content)} characters", file=sys.stderr)
        
        # Find all tables
        tables = soup.find_all('table')
        print(f"[MARKS EXTRACT] Found {len(tables)} tables on page", file=sys.stderr)
        
        # Process each table
        for table_idx, table in enumerate(tables):
            rows = table.find_all('tr')
            print(f"[MARKS EXTRACT] Table {table_idx}: {len(rows)} rows", file=sys.stderr)
            
            # Process each row (skip header rows)
            for i, row in enumerate(rows):
                cells = row.find_all('td')
                
                # Skip header row
                if i == 0:
                    print(f"[DEBUG] Skipping header row {i}", file=sys.stderr)
                    continue
                
                # Skip rows with insufficient cells, but try different cell arrangements
                if len(cells) < 2:
                    print(f"[DEBUG] Skipping row {i}: insufficient cells ({len(cells)})", file=sys.stderr)
                    continue
                
                # Try different cell arrangements for assessments
                assessments_cell = None
                if len(cells) >= 3:
                    assessments_cell = cells[2]  # Standard: Course, Type, Assessments
                elif len(cells) >= 4:
                    assessments_cell = cells[3]  # Alternative: Course, Type, Other, Assessments
                elif len(cells) >= 5:
                    assessments_cell = cells[4]  # Another alternative
                else:
                    # Try to find any cell that contains assessment patterns
                    for cell_idx, cell in enumerate(cells):
                        cell_text = cell.get_text(strip=True)
                        if any(pattern in cell_text for pattern in ['FT-', 'FP-', 'LLJ-', '/15.00', '/10.00']):
                            assessments_cell = cell
                            print(f"[DEBUG] Found assessments in cell {cell_idx}", file=sys.stderr)
                            break
                
                if not assessments_cell:
                    print(f"[DEBUG] Skipping row {i}: no assessment cell found", file=sys.stderr)
                    continue
                
                # Debug: Show row structure
                row_text = row.get_text(strip=True)
                if len(row_text) > 0:
                    print(f"[DEBUG] Row {i} preview: '{row_text[:50]}...'", file=sys.stderr)
                
                try:
                    # Extract course code and subject type from first two cells
                    course_code_cell = cells[0]
                    subject_type_cell = cells[1] if len(cells) > 1 else None
                    
                    # Debug: Print what's in each cell
                    print(f"\n[DEBUG] Row {i}:", file=sys.stderr)
                    print(f"  Cell 0 (Course): '{course_code_cell.get_text(strip=True)}'", file=sys.stderr)
                    print(f"  Cell 1 (Type): '{subject_type_cell.get_text(strip=True) if subject_type_cell else 'N/A'}'", file=sys.stderr)
                    print(f"  Assessments Cell: '{assessments_cell.get_text(strip=True)[:100]}...'", file=sys.stderr)
                    
                    # Extract course code using regex from the full text (handles "Regular" suffix)
                    course_code_text = course_code_cell.get_text(strip=True)
                    course_code_match = re.search(r'(\d{2}[A-Z]{3}\d{3}[A-Z])', course_code_text)
                    if not course_code_match:
                        print(f"  [SKIP] No valid course code found in: '{course_code_text}'", file=sys.stderr)
                        continue
                    
                    course_code = course_code_match.group(1)
                    subject_type = subject_type_cell.get_text(strip=True) if subject_type_cell else "Unknown"
                    
                    # Get course title from the course_titles dictionary
                    course_title = course_titles.get(course_code, "Unknown Course Title") if course_titles else "Unknown Course Title"
                    
                    # Debug: Show what's in the course_titles dictionary
                    print(f"  [DEBUG] Course titles dict has {len(course_titles) if course_titles else 0} entries", file=sys.stderr)
                    print(f"  [DEBUG] Looking for course_code: '{course_code}'", file=sys.stderr)
                    print(f"  [DEBUG] Found course_title: '{course_title}'", file=sys.stderr)
                    
                    print(f"  [OK] Processing course: {course_code} - {course_title} ({subject_type})", file=sys.stderr)
                    
                    # Extract assessments from the third cell
                    assessments = []
                    
                    # Method 1: Look for nested table in the assessments cell
                    nested_table = assessments_cell.find('table')
                    if nested_table:
                        print(f"  [DEBUG] Found nested table with {len(nested_table.find_all('tr'))} rows", file=sys.stderr)
                        # Process nested table rows
                        nested_rows = nested_table.find_all('tr')
                        for nested_row in nested_rows:
                            nested_cells = nested_row.find_all('td')
                            
                            for cell in nested_cells:
                                # Get the font element which contains the assessment data
                                font_element = cell.find('font')
                                if font_element:
                                    # Extract the strong element (assessment name/total) and the text after <br>
                                    strong_element = font_element.find('strong')
                                    if strong_element:
                                        assessment_info = strong_element.get_text(strip=True)  # e.g., "FT-II/15.00"
                                        
                                        # Get the text after <br> tag (marks obtained)
                                        br_tag = font_element.find('br')
                                        if br_tag:
                                            # Get the text that comes after the <br> tag
                                            marks_obtained = br_tag.next_sibling
                                            if marks_obtained:
                                                marks_obtained = str(marks_obtained).strip()
                                            else:
                                                # Alternative: get all text and split by line breaks
                                                all_text = font_element.get_text(strip=True)
                                                lines = all_text.split('\n')
                                                if len(lines) >= 2:
                                                    marks_obtained = lines[1].strip()
                                                else:
                                                    continue
                                        else:
                                            continue
                                        
                                        # Parse assessment info
                                        if '/' in assessment_info:
                                            assessment_name, total_marks = assessment_info.split('/', 1)
                                            assessment_name = assessment_name.strip()
                                            total_marks = total_marks.strip()
                                            
                                            # Format marks to 2 decimal places
                                            try:
                                                total_marks_float = float(total_marks)
                                                marks_obtained_float = float(marks_obtained)
                                                total_marks = f"{total_marks_float:.2f}"
                                                marks_obtained = f"{marks_obtained_float:.2f}"
                                            except ValueError:
                                                # If conversion fails, keep original values
                                                pass
                                            
                                            assessments.append({
                                                'assessment_name': assessment_name,
                                                'total_marks': total_marks,
                                                'marks_obtained': marks_obtained,
                                                'percentage': calculate_percentage(marks_obtained, total_marks)
                                            })
                                            print(f"    [OK] Found assessment: {assessment_name} = {marks_obtained}/{total_marks}", file=sys.stderr)
                    
                    # Method 2: If no nested table found, try to extract from cell text directly
                    if not assessments:
                        cell_text = assessments_cell.get_text(strip=True)
                        print(f"  [DEBUG] No nested table, trying direct text extraction: '{cell_text[:200]}...'", file=sys.stderr)
                        
                        # Multiple assessment patterns to try
                        assessment_patterns = [
                            r'([A-Z]+-[IVX]+)/(\d+\.?\d*)\s+(\d+\.?\d*)',  # FT-II/15.00 13.50
                            r'([A-Z]+-[IVX]+)/(\d+\.?\d*)\n(\d+\.?\d*)',   # FT-II/15.00\n13.50
                            r'([A-Z]+-[IVX]+)/(\d+\.?\d*).*?(\d+\.?\d*)',  # More flexible
                        ]
                        
                        for pattern_idx, pattern in enumerate(assessment_patterns):
                            matches = re.findall(pattern, cell_text)
                            print(f"  [DEBUG] Pattern {pattern_idx + 1} matches: {matches}", file=sys.stderr)
                            if matches:
                                for match in matches:
                                    assessment_name, total_marks, marks_obtained = match
                                    
                                    # Format marks to 2 decimal places
                                    try:
                                        total_marks_float = float(total_marks)
                                        marks_obtained_float = float(marks_obtained)
                                        total_marks = f"{total_marks_float:.2f}"
                                        marks_obtained = f"{marks_obtained_float:.2f}"
                                    except ValueError:
                                        pass
                                    
                                    assessments.append({
                                        'assessment_name': assessment_name,
                                        'total_marks': total_marks,
                                        'marks_obtained': marks_obtained,
                                        'percentage': calculate_percentage(marks_obtained, total_marks)
                                    })
                                    print(f"  [OK] Found assessment: {assessment_name} = {marks_obtained}/{total_marks}", file=sys.stderr)
                                break
                    
                    # Method 3: Look for any text that contains assessment-like patterns
                    if not assessments:
                        cell_text = assessments_cell.get_text(strip=True)
                        print(f"  [DEBUG] Trying fallback pattern matching...", file=sys.stderr)
                        
                        # Look for any pattern that looks like assessments
                        fallback_patterns = [
                            r'([A-Z]{2,}-[IVX]+)/(\d+\.?\d*)',  # Any assessment pattern
                            r'(\w+)/(\d+\.?\d*)',               # Any word/number pattern
                        ]
                        
                        for pattern in fallback_patterns:
                            matches = re.findall(pattern, cell_text)
                            if matches:
                                print(f"  [DEBUG] Fallback pattern found: {matches}", file=sys.stderr)
                                # Try to find corresponding marks
                                for match in matches:
                                    assessment_name, total_marks = match
                                    # Look for marks near this assessment
                                    marks_pattern = rf'{re.escape(assessment_name)}/{re.escape(total_marks)}.*?(\d+\.?\d*)'
                                    marks_match = re.search(marks_pattern, cell_text)
                                    if marks_match:
                                        marks_obtained = marks_match.group(1)
                                        
                                        # Format marks to 2 decimal places
                                        try:
                                            total_marks_float = float(total_marks)
                                            marks_obtained_float = float(marks_obtained)
                                            total_marks = f"{total_marks_float:.2f}"
                                            marks_obtained = f"{marks_obtained_float:.2f}"
                                        except ValueError:
                                            pass
                                        
                                        assessments.append({
                                            'assessment_name': assessment_name,
                                            'total_marks': total_marks,
                                            'marks_obtained': marks_obtained,
                                            'percentage': calculate_percentage(marks_obtained, total_marks)
                                        })
                                        print(f"  [OK] Fallback found: {assessment_name} = {marks_obtained}/{total_marks}", file=sys.stderr)
                                break
                    
                    if assessments:
                        marks_entry = {
                            'course_code': course_code,
                            'course_title': course_title,
                            'subject_type': subject_type,
                            'assessments': assessments,
                            'total_assessments': len(assessments)
                        }
                        
                        marks_data.append(marks_entry)
                        print(f"  [SUCCESS] {course_code}: {len(assessments)} assessments found", file=sys.stderr)
                    else:
                        print(f"  [WARN] {course_code}: No assessments found", file=sys.stderr)
                    
                except Exception as e:
                    print(f"[ERROR] Error processing row {i}: {e}", file=sys.stderr)
                    import traceback
                    traceback.print_exc(file=sys.stderr)
                    continue
    
    except Exception as e:
        print(f"[MARKS EXTRACT] Error extracting marks data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
    
    return marks_data


def calculate_percentage(obtained, total):
    """Calculate percentage from obtained and total marks, both formatted to 2 decimal places"""
    try:
        obtained_float = float(obtained)
        total_float = float(total)
        if total_float > 0:
            percentage = (obtained_float / total_float) * 100
            return f"{percentage:.2f}%"
        return "0.00%"
    except:
        return "N/A"


def create_marks_json(marks_data):
    """Create JSON structure with marks data"""
    
    # Calculate summary statistics
    total_courses = len(marks_data)
    total_assessments = sum(entry['total_assessments'] for entry in marks_data)
    
    # Group by subject type
    theory_courses = [entry for entry in marks_data if entry['subject_type'].lower() == 'theory']
    lab_courses = [entry for entry in marks_data if entry['subject_type'].lower() == 'lab']
    other_courses = [entry for entry in marks_data if entry['subject_type'].lower() not in ['theory', 'lab']]
    
    # Create the complete JSON structure
    marks_json = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source": "SRM Academia Portal - Internal Marks",
            "academic_year": "2025-26 ODD",
            "institution": "SRM Institute of Science and Technology",
            "college": "College of Engineering and Technology",
            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        },
        "summary": {
            "total_courses": total_courses,
            "theory_courses": len(theory_courses),
            "lab_courses": len(lab_courses),
            "other_courses": len(other_courses),
            "total_assessments": total_assessments
        },
        "courses": {
            "theory": theory_courses,
            "lab": lab_courses,
            "other": other_courses
        },
        "all_courses": marks_data
    }
    
    return marks_json


def api_get_marks_data(email, password):
    """API function to get marks data with session management"""
    scraper = None
    try:
        print(f"[API] Getting marks data for: {email}", file=sys.stderr)
        
        # Initialize scraper with session management
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True)
        
        html_content = None
        # Try to get data with existing session first
        if scraper.is_session_valid():
            print("[API] Valid session found - trying to get data without login", file=sys.stderr)
            html_content = get_marks_page_html(scraper)
        
        # If session was invalid or data fetch failed, attempt login
        if html_content is None:
            print("[API] Session invalid or expired - attempting login", file=sys.stderr)
            if not scraper.login(email, password):
                print("[API] Login failed!", file=sys.stderr)
                return {"success": False, "error": "Login failed"}
            print("[API] Login successful!", file=sys.stderr)
            html_content = get_marks_page_html(scraper)

        if not html_content:
            print("[API] Failed to get marks HTML content after all attempts", file=sys.stderr)
            return {"success": False, "error": "Failed to get marks data"}
        
        print(f"[API] Got HTML content ({len(html_content)} characters)", file=sys.stderr)
        
        # Extract course titles first
        print("[API] Extracting course titles...", file=sys.stderr)
        course_titles = extract_course_titles_from_html(html_content)
        
        # Extract marks data with course titles
        print("[API] Extracting marks data...", file=sys.stderr)
        marks_data = extract_marks_data_from_html(html_content, course_titles)
        
        if marks_data:
            print(f"[API] Successfully extracted {len(marks_data)} marks entries", file=sys.stderr)
            marks_json = create_marks_json(marks_data)
            return {
                "success": True,
                "data": marks_json,
                "type": "marks",
                "count": len(marks_data),
                "cached": False
            }
        else:
            print("[API] No marks data extracted", file=sys.stderr)
            empty_marks_json = {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "source": "SRM Academia Portal - Internal Marks",
                    "academic_year": "2025-26 ODD",
                    "institution": "SRM Institute of Science and Technology",
                    "college": "College of Engineering and Technology",
                    "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                "summary": {
                    "total_courses": 0,
                    "theory_courses": 0,
                    "lab_courses": 0,
                    "other_courses": 0,
                    "total_assessments": 0
                },
                "courses": {
                    "theory": [],
                    "lab": [],
                    "other": []
                },
                "all_courses": []
            }
            return {
                "success": True,
                "data": empty_marks_json,
                "type": "marks",
                "count": 0,
                "cached": False
            }
        
    except Exception as e:
        print(f"[API] Error getting marks data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[API] Scraper closed", file=sys.stderr)
            except Exception as e:
                print(f"[API] Error closing scraper: {e}", file=sys.stderr)


def get_calendar_data_with_scraper(scraper, email, password, force_refresh=False, trust_logged_in=False):
    """
    Get calendar data using existing scraper instance
    
    Args:
        scraper: Selenium scraper instance
        email: User email
        password: User password (optional if trust_logged_in=True)
        force_refresh: Force refresh cached data
        trust_logged_in: If True, we just logged in - trust browser state, fetch directly (NO CHECKS)
    """
    try:
        print(f"[UNIFIED API] Getting calendar data for: {email}", file=sys.stderr)

        # Check cache first (unless force refresh)
        if not force_refresh and is_cache_valid():
            cached_data = get_cached_calendar_data()
            if cached_data:
                print(f"[UNIFIED API] Using cached calendar data ({len(cached_data)} entries)", file=sys.stderr)
                return {
                    "success": True,
                    "data": cached_data,
                    "type": "calendar",
                    "count": len(cached_data),
                    "cached": True,
                    "cache_timestamp": datetime.now().isoformat()
                }

        print("[UNIFIED API] Cache expired or empty - fetching fresh calendar data", file=sys.stderr)

        html_content = None
        # ✅ OPTIMIZED: If we trust login (just logged in), fetch directly - NO CHECKS, NO VERIFICATION
        if trust_logged_in:
            print("[OPTIMIZED] Trusting login state - fetching calendar directly (no checks)", file=sys.stderr)
            html_content = scraper.get_calendar_data()  # Direct fetch - trust browser is logged in
        else:
            # Backward compatibility: Check login only if we didn't login at top level
            if scraper.is_logged_in():
                print("[UNIFIED API] Browser already logged in - fetching calendar data directly", file=sys.stderr)
                html_content = scraper.get_calendar_data()
            else:
                # Browser not logged in - need to login first
                print("[UNIFIED API] Browser not logged in - attempting login for calendar", file=sys.stderr)
                if not scraper.login(email, password):
                    print("[UNIFIED API] Login failed for calendar!", file=sys.stderr)
                    return {"success": False, "error": "Login failed"}
                print("[UNIFIED API] Login successful for calendar!", file=sys.stderr)
                html_content = scraper.get_calendar_data()
        
        # If data fetch failed after login, return error
        if html_content is None:
            print("[UNIFIED API] Failed to get calendar HTML content after login", file=sys.stderr)
            return {"success": False, "error": "Failed to get calendar data after login"}

        if not html_content:
            print("[UNIFIED API] Failed to get calendar HTML content after all attempts", file=sys.stderr)
            return {"success": False, "error": "Failed to get calendar data"}

        print(f"[UNIFIED API] Got calendar HTML content ({len(html_content)} characters)", file=sys.stderr)
        calendar_data = extract_calendar_data_from_html(html_content)

        if calendar_data:
            print(f"[UNIFIED API] Successfully extracted {len(calendar_data)} calendar entries", file=sys.stderr)
            save_calendar_cache(calendar_data)
            return {
                "success": True,
                "data": calendar_data,
                "type": "calendar",
                "count": len(calendar_data),
                "cached": False,
                "fresh_data": True
            }
        else:
            print("[UNIFIED API] No calendar data extracted", file=sys.stderr)
            return {
                "success": True,
                "data": [],
                "type": "calendar",
                "count": 0,
                "cached": False
            }
        
    except Exception as e:
        print(f"[UNIFIED API] Error getting calendar data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)

        if not force_refresh:
            cached_data = get_cached_calendar_data()
            if cached_data:
                print("[UNIFIED API] Scraping failed, using stale cache as fallback", file=sys.stderr)
                return {
                    "success": True,
                    "data": cached_data,
                    "type": "calendar",
                    "count": len(cached_data),
                    "cached": True,
                    "stale": True,
                    "fallback": True
                }
        return {"success": False, "error": f"API Error: {str(e)}"}


def get_attendance_and_marks_data_with_scraper(scraper, email, password, trust_logged_in=False):
    """
    Get both attendance and marks data from the same page (using individual function logic)
    
    Args:
        scraper: Selenium scraper instance
        email: User email
        password: User password (optional if trust_logged_in=True)
        trust_logged_in: If True, we just logged in - trust browser state, fetch directly (NO CHECKS)
    """
    try:
        print(f"[UNIFIED API] Getting attendance and marks data for: {email}", file=sys.stderr)
        
        # ✅ OPTIMIZED: If we trust login (just logged in), fetch directly - NO CHECKS, NO VERIFICATION
        html_content = None
        if trust_logged_in:
            print("[OPTIMIZED] Trusting login state - fetching attendance/marks directly (no checks)", file=sys.stderr)
            # Direct fetch - trust browser is logged in
            html_content = get_marks_page_html(scraper)
        else:
            # Backward compatibility: Check login only if we didn't login at top level
            if scraper.is_logged_in():
                print("[UNIFIED API] Browser already logged in - fetching attendance/marks data directly", file=sys.stderr)
                html_content = get_marks_page_html(scraper)
            else:
                # Browser not logged in - need to login first
                print("[UNIFIED API] Browser not logged in - attempting login for attendance/marks", file=sys.stderr)
                if not scraper.login(email, password):
                    print("[UNIFIED API] Login failed!", file=sys.stderr)
                    return {
                        "attendance": {"success": False, "error": "Login failed"},
                        "marks": {"success": False, "error": "Login failed"}
                    }
                print("[UNIFIED API] Login successful!", file=sys.stderr)
                html_content = get_marks_page_html(scraper)

        if not html_content:
            print("[UNIFIED API] Failed to get HTML content after all attempts", file=sys.stderr)
            return {
                "attendance": {"success": False, "error": "Failed to get data - likely on login page"},
                "marks": {"success": False, "error": "Failed to get data - likely on login page"}
            }
        
        print(f"[UNIFIED API] Got HTML content ({len(html_content)} characters)", file=sys.stderr)
        
        # ✅ CRITICAL: Check if we're on login page (even though get_marks_page_html should have caught this)
        if "Login" in scraper.driver.title or "signinFrame" in html_content[:5000]:
            print("[UNIFIED API] ERROR: Login page detected in HTML content - marking as failed", file=sys.stderr)
            return {
                "attendance": {"success": False, "error": "Session expired - redirected to login page"},
                "marks": {"success": False, "error": "Session expired - redirected to login page"}
            }
        
        # Extract attendance data (same as individual function)
        print("[UNIFIED API] Extracting attendance data...", file=sys.stderr)
        attendance_data = extract_attendance_data_from_html(html_content)
        
        # Extract semester from HTML
        semester = extract_semester_from_html(html_content)
        print(f"[UNIFIED API] Extracted semester: {semester}", file=sys.stderr)
        
        if attendance_data:
            print(f"[UNIFIED API] Successfully extracted {len(attendance_data)} attendance entries", file=sys.stderr)
            attendance_json = create_attendance_json(attendance_data, semester)
            attendance_result = {
                "success": True,
                "data": attendance_json,
                "type": "attendance",
                "count": len(attendance_data),
                "semester": semester,
                "cached": False
            }
        else:
            # ✅ FIXED: If no data extracted AND we might be on login page, mark as failed
            print("[UNIFIED API] No attendance data extracted", file=sys.stderr)
            # Check if this looks like a login page (no data + login indicators)
            is_likely_login_page = "signinFrame" in html_content[:5000] or len(html_content) < 10000
            if is_likely_login_page:
                print("[UNIFIED API] WARNING: No data and page looks like login page - marking as failed", file=sys.stderr)
                attendance_result = {
                    "success": False,
                    "error": "No data extracted - likely on login page",
                    "type": "attendance",
                    "count": 0,
                    "cached": False
                }
            else:
                # Legitimately no data (empty table)
                attendance_result = {
                    "success": True,
                    "data": {"all_subjects": [], "summary": {"total_subjects": 0}},
                    "type": "attendance",
                    "count": 0,
                    "semester": semester,
                    "cached": False
                }
        
        # Extract marks data (same as individual function) - USE THE SAME HTML BUT WITH PROPER VALIDATION
        print("[UNIFIED API] Extracting marks data...", file=sys.stderr)
        
        # Extract course titles first (same as individual function)
        print("[UNIFIED API] Extracting course titles...", file=sys.stderr)
        course_titles = extract_course_titles_from_html(html_content)
        
        # Extract marks data with course titles (same as individual function)
        marks_data = extract_marks_data_from_html(html_content, course_titles)
        
        if marks_data:
            print(f"[UNIFIED API] Successfully extracted {len(marks_data)} marks entries", file=sys.stderr)
            marks_json = create_marks_json(marks_data)
            marks_result = {
                "success": True,
                "data": marks_json,
                "type": "marks",
                "count": len(marks_data),
                "cached": False
            }
        else:
            print("[UNIFIED API] No marks data extracted", file=sys.stderr)
            # ✅ FIXED: Check if we're on login page - if so, mark as failed
            is_likely_login_page = "signinFrame" in html_content[:5000] or len(html_content) < 10000
            if is_likely_login_page:
                print("[UNIFIED API] WARNING: No marks data and page looks like login page - marking as failed", file=sys.stderr)
                marks_result = {
                    "success": False,
                    "error": "No data extracted - likely on login page",
                    "type": "marks",
                    "count": 0,
                    "cached": False
                }
            else:
                # Legitimately no marks data
                empty_marks_json = {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "source": "SRM Academia Portal - Internal Marks",
                    "academic_year": "2025-26 ODD",
                    "institution": "SRM Institute of Science and Technology",
                    "college": "College of Engineering and Technology",
                    "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                "summary": {
                    "total_courses": 0,
                    "theory_courses": 0,
                    "lab_courses": 0,
                    "other_courses": 0,
                    "total_assessments": 0
                },
                "courses": {
                    "theory": [],
                    "lab": [],
                    "other": []
                },
                "all_courses": []
            }
            marks_result = {
                "success": True,
                "data": empty_marks_json,
                "type": "marks",
                "count": 0,
                "cached": False
            }
        
        return {
            "attendance": attendance_result,
            "marks": marks_result
        }
        
    except Exception as e:
        print(f"[UNIFIED API] Exception in get_attendance_and_marks_data_with_scraper: {e}", file=sys.stderr)
        return {
            "attendance": {"success": False, "error": f"Exception: {str(e)}"},
            "marks": {"success": False, "error": f"Exception: {str(e)}"}
        }


def get_timetable_data_with_scraper(scraper, email, password, trust_logged_in=False):
    """
    Get timetable data using existing scraper instance
    
    Args:
        scraper: Selenium scraper instance
        email: User email
        password: User password (optional if trust_logged_in=True)
        trust_logged_in: If True, we just logged in - trust browser state, fetch directly (NO CHECKS)
    """
    try:
        print(f"[UNIFIED API] Getting timetable data for: {email}", file=sys.stderr)
        
        html_content = None
        # ✅ OPTIMIZED: If we trust login (just logged in), fetch directly - NO CHECKS, NO VERIFICATION
        if trust_logged_in:
            print("[OPTIMIZED] Trusting login state - fetching timetable directly (no checks)", file=sys.stderr)
            html_content = get_timetable_page_html(scraper)  # Direct fetch - trust browser is logged in
        else:
            # Backward compatibility: Check login only if we didn't login at top level
            if scraper.is_logged_in():
                print("[UNIFIED API] Browser already logged in - fetching timetable data directly", file=sys.stderr)
                html_content = get_timetable_page_html(scraper)
            else:
                # Browser not logged in - need to login first
                print("[UNIFIED API] Browser not logged in - attempting login for timetable", file=sys.stderr)
                if not scraper.login(email, password):
                    print("[UNIFIED API] Login failed for timetable!", file=sys.stderr)
                    return {"success": False, "error": "Login failed"}
                print("[UNIFIED API] Login successful for timetable!", file=sys.stderr)
                html_content = get_timetable_page_html(scraper)
        
        # ✅ PHASE 2 FIX: Improved error detection for login page redirects
        if html_content is None:
            print("[UNIFIED API] Failed to get timetable HTML content", file=sys.stderr)
            # Check if we're on login page (session expired)
            try:
                if hasattr(scraper, 'driver') and scraper.driver:
                    current_title = scraper.driver.title
                    if "Login" in current_title:
                        print("[UNIFIED API] Login page detected - session expired", file=sys.stderr)
                        return {"success": False, "error": "Session expired - please re-authenticate"}
            except:
                pass
            return {"success": False, "error": "Failed to get timetable data"}

        if not html_content:
            print("[UNIFIED API] Failed to get timetable HTML content after all attempts", file=sys.stderr)
            return {"success": False, "error": "Failed to get timetable data"}
        
        # ✅ PHASE 2 FIX: Validate we got actual timetable data, not login page
        if len(html_content) < 10000:  # Login pages are much smaller
            print(f"[UNIFIED API] WARNING: Page source too small ({len(html_content)} chars) - might be login page", file=sys.stderr)
            if "Login" in html_content or "signinFrame" in html_content:
                print("[UNIFIED API] ERROR: Confirmed login page - session expired", file=sys.stderr)
                return {"success": False, "error": "Session expired during timetable fetch - please re-authenticate"}
        
        print(f"[UNIFIED API] Got timetable HTML content ({len(html_content)} characters)", file=sys.stderr)
        
        # Extract course data and batch number
        courses, batch_number = extract_timetable_data_from_html(html_content)
        
        print(f"[UNIFIED API] Extracted {len(courses) if courses else 0} courses", file=sys.stderr)
        print(f"[UNIFIED API] Batch number: {batch_number}", file=sys.stderr)
        
        if not courses:
            print("[UNIFIED API] No timetable data extracted - possible causes:", file=sys.stderr)
            print("  - No timetable assigned to student", file=sys.stderr)
            print("  - Timetable page structure changed", file=sys.stderr)
            print("  - HTML parsing failed", file=sys.stderr)
            print("  - Session expired during extraction", file=sys.stderr)
            print("[UNIFIED API] Returning proper empty timetable structure", file=sys.stderr)
            
            return {
                "success": True,
                "data": {
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "source": "SRM Academia Portal",
                        "academic_year": "2025-26 ODD",
                        "format": "Day Order (DO) Timetable",
                        "batch_number": batch_number,
                        "batch_name": "No Timetable Available"
                    },
                    "time_slots": [],
                    "slot_mapping": {},
                    "timetable": {}
                },
                "type": "timetable",
                "count": 0,
                "cached": False
            }
        
        # Display batch number if found
        if batch_number:
            print(f"[UNIFIED API] Batch number detected: {batch_number}", file=sys.stderr)
        
        # Create slot mapping from courses
        from timetable_scraper import create_slot_mapping, expand_slot_mapping
        slot_mapping = create_slot_mapping(courses)
        expanded_slot_mapping = expand_slot_mapping(slot_mapping)
        
        # Create timetable JSON with proper slot mapping
        timetable_json = create_do_timetable_json(expanded_slot_mapping, batch_number)
        
        print(f"[UNIFIED API] Successfully extracted {len(courses)} timetable entries", file=sys.stderr)
        return {
            "success": True,
            "data": timetable_json,
            "type": "timetable",
            "count": len(courses),
            "cached": False
        }
        
    except Exception as e:
        print(f"[UNIFIED API] Error getting timetable data: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"API Error: {str(e)}"}


def api_get_all_data(email, password=None, force_refresh=False):
    """
    API function to get all data types in a unified response using single browser session.
    
    Args:
        email: User email (required)
        password: User password (optional - will use existing session if not provided)
        force_refresh: Force refresh cached data
    
    Returns:
        Dict with success status and data for all types (calendar, attendance, marks, timetable)
    """
    print(f"[UNIFIED API] Getting all data for: {email} (single browser session)", file=sys.stderr)
    print(f"[UNIFIED API] Password provided: {password is not None}", file=sys.stderr)
    
    scraper = None
    try:
        # Initialize single scraper instance with per-user session
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True, user_email=email)
        
        # Track results for each data type
        results = {}
        successful_data_types = 0
        total_data_types = 4  # calendar, attendance, marks, timetable
        
        # ✅ OPTIMIZATION: Track if we just logged in - if so, trust browser state completely
        login_performed = False
        
        # Check if session is valid
        print(f"[UNIFIED API] Checking session validity...", file=sys.stderr)
        session_valid = scraper.is_session_valid()
        print(f"[UNIFIED API] Session valid: {session_valid}", file=sys.stderr)
        
        # SMART SESSION LOGIC: Check session validity FIRST, then login if needed
        if session_valid:
            # Session exists and is valid → Already logged in from session
            login_performed = False
            print("[UNIFIED API] Valid session found - already logged in", file=sys.stderr)
        elif password is not None and password:
            # No valid session → Login with password
            print(f"[UNIFIED API] Logging in with provided credentials...", file=sys.stderr)
            print(f"[UNIFIED API] Email: {email}, Password length: {len(password) if password else 0}", file=sys.stderr)
            login_result = scraper.login(email, password)
            print(f"[UNIFIED API] Login method returned: {login_result}", file=sys.stderr)
            if not login_result:
                print("[UNIFIED API] Login failed! Check detailed logs above for reason.", file=sys.stderr)
                return {"success": False, "error": "login_failed", "message": "Invalid credentials"}
            
            # ✅ CRITICAL: We just logged in successfully - trust browser state, no more checks
            login_performed = True
            print("[UNIFIED API] Login successful - browser is logged in, fetching data directly", file=sys.stderr)
        else:
            # No valid session and no password provided
            print(f"[UNIFIED API] No valid session and no password provided", file=sys.stderr)
            print(f"[UNIFIED API] Password value: {password}, Password is None: {password is None}, Password truthy: {bool(password)}", file=sys.stderr)
            return {
                "success": False,
                "error": "session_expired",
                "message": "Session expired. Please re-authenticate with your password."
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[UNIFIED API] Fetching calendar data (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            calendar_result = get_calendar_data_with_scraper(scraper, email, password, force_refresh, trust_logged_in=login_performed)
            results['calendar'] = calendar_result
            
            if calendar_result.get('success', False):
                successful_data_types += 1
                print(f"[UNIFIED API] ✓ calendar data fetched successfully", file=sys.stderr)
            else:
                print(f"[UNIFIED API] ✗ calendar data failed: {calendar_result.get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[UNIFIED API] ✗ calendar data error: {e}", file=sys.stderr)
            results['calendar'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "calendar",
                "count": 0
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[UNIFIED API] Fetching attendance and marks data together (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            combined_result = get_attendance_and_marks_data_with_scraper(scraper, email, password, trust_logged_in=login_performed)
            results['attendance'] = combined_result['attendance']
            results['marks'] = combined_result['marks']
            
            # Count successful data types
            if combined_result['attendance'].get('success', False):
                successful_data_types += 1
                print(f"[UNIFIED API] ✓ attendance data fetched successfully", file=sys.stderr)
            else:
                print(f"[UNIFIED API] ✗ attendance data failed: {combined_result['attendance'].get('error', 'Unknown error')}", file=sys.stderr)
                
            if combined_result['marks'].get('success', False):
                successful_data_types += 1
                print(f"[UNIFIED API] ✓ marks data fetched successfully", file=sys.stderr)
            else:
                print(f"[UNIFIED API] ✗ marks data failed: {combined_result['marks'].get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[UNIFIED API] ✗ attendance/marks data error: {e}", file=sys.stderr)
            results['attendance'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "attendance",
                "count": 0
            }
            results['marks'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "marks",
                "count": 0
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[UNIFIED API] Fetching timetable data (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            timetable_result = get_timetable_data_with_scraper(scraper, email, password, trust_logged_in=login_performed)
            results['timetable'] = timetable_result
            
            if timetable_result.get('success', False):
                data = timetable_result.get('data', {})
                print(f"[UNIFIED API] ✓ timetable data fetched successfully", file=sys.stderr)
                print(f"[UNIFIED API] Timetable data type: {type(data)}", file=sys.stderr)
                print(f"[UNIFIED API] Timetable data keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}", file=sys.stderr)
                if isinstance(data, dict) and 'timetable' in data:
                    print(f"[UNIFIED API] Timetable.timetable keys: {list(data['timetable'].keys())}", file=sys.stderr)
                successful_data_types += 1
            else:
                print(f"[UNIFIED API] ✗ timetable data failed: {timetable_result.get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[UNIFIED API] ✗ timetable data error: {e}", file=sys.stderr)
            results['timetable'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "timetable",
                "count": 0
            }
        
        # Calculate success rate
        success_rate = f"{(successful_data_types / total_data_types) * 100:.1f}%"
        
        # Determine overall success
        overall_success = successful_data_types > 0  # Success if at least one data type worked
        
        # Create unified response
        unified_response = {
            "success": overall_success,
            "data": {
                "calendar": results.get('calendar', {"success": False, "error": "Not attempted"}),
                "attendance": results.get('attendance', {"success": False, "error": "Not attempted"}),
                "marks": results.get('marks', {"success": False, "error": "Not attempted"}),
                "timetable": results.get('timetable', {"success": False, "error": "Not attempted"})
            },
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "source": "SRM Academia Portal - Unified Data (Phase 1+ Optimized)",
                "email": email,
                "total_data_types": total_data_types,
                "successful_data_types": successful_data_types,
                "success_rate": success_rate,
                "cached": False,
                "force_refresh": force_refresh,
                "single_browser_session": True,
                "optimizations": [
                    "reduced_sleep_times",
                    "combined_attendance_marks_fetch",
                    "performance_chrome_options",
                    "single_page_reuse",
                    "removed_document_ready_waits",
                    "reduced_login_wait",
                    "optimized_wait_conditions",
                    "aggressive_chrome_performance"
                ]
            }
        }
        
        # Add error field if overall success is False
        if not overall_success:
            unified_response["error"] = f"All data types failed. Success rate: {success_rate}"
        
        print(f"[UNIFIED API] Completed: {successful_data_types}/{total_data_types} data types successful ({success_rate})", file=sys.stderr)
        
        return unified_response
        
    except Exception as e:
        print(f"[UNIFIED API] Error in unified data fetch: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"Unified API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[UNIFIED API] Single browser session closed", file=sys.stderr)
            except Exception as e:
                print(f"[UNIFIED API] Error closing scraper: {e}", file=sys.stderr)


def api_get_static_data(email, password=None, force_refresh=False):
    """
    API function to get static (cacheable) data: Calendar + Timetable.
    This data should be cached on the frontend and fetched only once.
    
    Args:
        email: User email (required)
        password: User password (optional - will use existing session if not provided)
        force_refresh: Force refresh cached data
    
    Returns:
        Dict with success status and data for calendar and timetable
    """
    print(f"[STATIC DATA API] Getting static data (Calendar + Timetable) for: {email}", file=sys.stderr)
    print(f"[STATIC DATA API] Password provided: {password is not None}", file=sys.stderr)
    
    scraper = None
    start_time = time.time()
    try:
        # Initialize single scraper instance with per-user session
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True, user_email=email)
        
        # Track results
        results = {}
        successful_data_types = 0
        total_data_types = 2  # calendar, timetable
        
        # ✅ OPTIMIZATION: Track if we just logged in - if so, trust browser state completely
        login_performed = False
        
        # Check if session is valid
        print(f"[STATIC DATA API] Checking session validity...", file=sys.stderr)
        session_valid = scraper.is_session_valid()
        print(f"[STATIC DATA API] Session valid: {session_valid}", file=sys.stderr)
        
        # SMART SESSION LOGIC: Check session validity FIRST, then login if needed
        if session_valid:
            # Session exists and is valid → Already logged in from session
            login_performed = False
            print("[STATIC DATA API] Valid session found - already logged in", file=sys.stderr)
        elif password is not None and password:
            # No valid session → Login with password
            print(f"[STATIC DATA API] Logging in with provided credentials...", file=sys.stderr)
            print(f"[STATIC DATA API] Email: {email}, Password length: {len(password) if password else 0}", file=sys.stderr)
            login_result = scraper.login(email, password)
            print(f"[STATIC DATA API] Login method returned: {login_result}", file=sys.stderr)
            if not login_result:
                print("[STATIC DATA API] Login failed! Check detailed logs above for reason.", file=sys.stderr)
                return {"success": False, "error": "login_failed", "message": "Invalid credentials"}
            
            # ✅ CRITICAL: We just logged in successfully - trust browser state, no more checks
            login_performed = True
            print("[STATIC DATA API] Login successful - browser is logged in, fetching data directly", file=sys.stderr)
        else:
            # No valid session and no password provided
            print(f"[STATIC DATA API] No valid session and no password provided", file=sys.stderr)
            print(f"[STATIC DATA API] Password value: {password}, Password is None: {password is None}, Password truthy: {bool(password)}", file=sys.stderr)
            return {
                "success": False,
                "error": "session_expired",
                "message": "Session expired. Please re-authenticate with your password."
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[STATIC DATA API] Fetching calendar data (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            calendar_result = get_calendar_data_with_scraper(scraper, email, password, force_refresh, trust_logged_in=login_performed)
            results['calendar'] = calendar_result
            
            if calendar_result.get('success', False):
                successful_data_types += 1
                print(f"[STATIC DATA API] ✓ calendar data fetched successfully", file=sys.stderr)
            else:
                print(f"[STATIC DATA API] ✗ calendar data failed: {calendar_result.get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[STATIC DATA API] ✗ calendar data error: {e}", file=sys.stderr)
            results['calendar'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "calendar",
                "count": 0
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[STATIC DATA API] Fetching timetable data (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            timetable_result = get_timetable_data_with_scraper(scraper, email, password, trust_logged_in=login_performed)
            results['timetable'] = timetable_result
            
            if timetable_result.get('success', False):
                successful_data_types += 1
                print(f"[STATIC DATA API] ✓ timetable data fetched successfully", file=sys.stderr)
            else:
                print(f"[STATIC DATA API] ✗ timetable data failed: {timetable_result.get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[STATIC DATA API] ✗ timetable data error: {e}", file=sys.stderr)
            results['timetable'] = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "timetable",
                "count": 0
            }
        
        # Calculate fetch duration
        fetch_duration = time.time() - start_time
        
        # Determine overall success
        overall_success = successful_data_types > 0  # Success if at least one data type worked
        
        # Create response with metadata indicating this is cacheable static data
        response = {
            "success": overall_success,
            "data": {
                "calendar": results.get('calendar', {"success": False, "error": "Not attempted"}),
                "timetable": results.get('timetable', {"success": False, "error": "Not attempted"})
            },
            "metadata": {
                "type": "static_data",
                "cacheable": True,
                "cache_hint": "frontend_cache",
                "generated_at": datetime.now().isoformat(),
                "fetch_duration_seconds": round(fetch_duration, 2),
                "source": "SRM Academia Portal - Static Data (Calendar + Timetable)",
                "email": email,
                "total_data_types": total_data_types,
                "successful_data_types": successful_data_types,
                "force_refresh": force_refresh,
                "single_browser_session": True,
                "phase_2_optimizations": [
                    "skip_redundant_login_checks",
                    "session_state_reuse",
                    "optimized_helper_functions"
                ]
            }
        }
        
        # Add error field if overall success is False
        if not overall_success:
            response["error"] = f"All static data types failed. Success rate: {(successful_data_types / total_data_types) * 100:.1f}%"
        
        print(f"[STATIC DATA API] Completed: {successful_data_types}/{total_data_types} data types successful in {fetch_duration:.2f}s", file=sys.stderr)
        
        return response
        
    except Exception as e:
        print(f"[STATIC DATA API] Error in static data fetch: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"Static Data API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[STATIC DATA API] Browser session closed", file=sys.stderr)
            except Exception as e:
                print(f"[STATIC DATA API] Error closing scraper: {e}", file=sys.stderr)


def api_get_dynamic_data(email, password=None):
    """
    API function to get dynamic (non-cacheable) data: Attendance + Marks.
    This data should be fetched fresh on every request.
    
    Args:
        email: User email (required)
        password: User password (optional - will use existing session if not provided)
    
    Returns:
        Dict with success status and data for attendance and marks
    """
    print(f"[DYNAMIC DATA API] Getting dynamic data (Attendance + Marks) for: {email}", file=sys.stderr)
    print(f"[DYNAMIC DATA API] Password provided: {password is not None}", file=sys.stderr)
    
    scraper = None
    start_time = time.time()
    try:
        # Initialize single scraper instance with per-user session
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True, user_email=email)
        
        # ✅ OPTIMIZATION: Track if we just logged in - if so, trust browser state completely
        login_performed = False
        
        # Check if session is valid
        print(f"[DYNAMIC DATA API] Checking session validity...", file=sys.stderr)
        session_valid = scraper.is_session_valid()
        print(f"[DYNAMIC DATA API] Session valid: {session_valid}", file=sys.stderr)
        
        # SMART SESSION LOGIC: Check session validity FIRST, then login if needed
        if session_valid:
            # Session exists and is valid → Already logged in from session
            login_performed = False
            print("[DYNAMIC DATA API] Valid session found - already logged in", file=sys.stderr)
        elif password is not None and password:
            # No valid session → Login with password
            print(f"[DYNAMIC DATA API] Logging in with provided credentials...", file=sys.stderr)
            print(f"[DYNAMIC DATA API] Email: {email}, Password length: {len(password) if password else 0}", file=sys.stderr)
            login_result = scraper.login(email, password)
            print(f"[DYNAMIC DATA API] Login method returned: {login_result}", file=sys.stderr)
            if not login_result:
                print("[DYNAMIC DATA API] Login failed! Check detailed logs above for reason.", file=sys.stderr)
                return {"success": False, "error": "login_failed", "message": "Invalid credentials"}
            
            # ✅ CRITICAL: We just logged in successfully - trust browser state, no more checks
            login_performed = True
            print("[DYNAMIC DATA API] Login successful - browser is logged in, fetching data directly", file=sys.stderr)
        else:
            # No valid session and no password provided
            print(f"[DYNAMIC DATA API] No valid session and no password provided", file=sys.stderr)
            print(f"[DYNAMIC DATA API] Password value: {password}, Password is None: {password is None}, Password truthy: {bool(password)}", file=sys.stderr)
            return {
                "success": False,
                "error": "session_expired",
                "message": "Session expired. Please re-authenticate with your password."
            }
        
        # ✅ OPTIMIZED: After login, we KNOW browser is logged in - fetch directly, NO CHECKS
        print(f"[DYNAMIC DATA API] Fetching attendance and marks data together (trust_logged_in={login_performed})...", file=sys.stderr)
        try:
            combined_result = get_attendance_and_marks_data_with_scraper(scraper, email, password, trust_logged_in=login_performed)
            attendance_result = combined_result['attendance']
            marks_result = combined_result['marks']
            
            # Track success
            successful_data_types = 0
            total_data_types = 2  # attendance, marks
            
            if attendance_result.get('success', False):
                successful_data_types += 1
                print(f"[DYNAMIC DATA API] ✓ attendance data fetched successfully", file=sys.stderr)
            else:
                print(f"[DYNAMIC DATA API] ✗ attendance data failed: {attendance_result.get('error', 'Unknown error')}", file=sys.stderr)
                
            if marks_result.get('success', False):
                successful_data_types += 1
                print(f"[DYNAMIC DATA API] ✓ marks data fetched successfully", file=sys.stderr)
            else:
                print(f"[DYNAMIC DATA API] ✗ marks data failed: {marks_result.get('error', 'Unknown error')}", file=sys.stderr)
                
        except Exception as e:
            print(f"[DYNAMIC DATA API] ✗ attendance/marks data error: {e}", file=sys.stderr)
            attendance_result = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "attendance",
                "count": 0
            }
            marks_result = {
                "success": False,
                "error": f"Exception: {str(e)}",
                "type": "marks",
                "count": 0
            }
            successful_data_types = 0
            total_data_types = 2
        
        # Calculate fetch duration
        fetch_duration = time.time() - start_time
        
        # Determine overall success
        overall_success = successful_data_types > 0  # Success if at least one data type worked
        
        # Create response with metadata indicating this is non-cacheable dynamic data
        response = {
            "success": overall_success,
            "data": {
                "attendance": attendance_result,
                "marks": marks_result
            },
            "metadata": {
                "type": "dynamic_data",
                "cacheable": False,
                "cache_hint": "no_cache",
                "generated_at": datetime.now().isoformat(),
                "fetch_duration_seconds": round(fetch_duration, 2),
                "source": "SRM Academia Portal - Dynamic Data (Attendance + Marks)",
                "email": email,
                "total_data_types": total_data_types,
                "successful_data_types": successful_data_types,
                "single_browser_session": True,
                "phase_2_optimizations": [
                    "skip_redundant_login_checks",
                    "session_state_reuse",
                    "optimized_helper_functions"
                ]
            }
        }
        
        # Add error field if overall success is False
        if not overall_success:
            response["error"] = f"All dynamic data types failed. Success rate: {(successful_data_types / total_data_types) * 100:.1f}%"
        
        print(f"[DYNAMIC DATA API] Completed: {successful_data_types}/{total_data_types} data types successful in {fetch_duration:.2f}s", file=sys.stderr)
        
        return response
        
    except Exception as e:
        print(f"[DYNAMIC DATA API] Error in dynamic data fetch: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {"success": False, "error": f"Dynamic Data API Error: {str(e)}"}
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[DYNAMIC DATA API] Browser session closed", file=sys.stderr)
            except Exception as e:
                print(f"[DYNAMIC DATA API] Error closing scraper: {e}", file=sys.stderr)


def api_validate_credentials(email, password):
    """
    API function to validate user credentials via portal login.
    Creates a session for the user if validation is successful.
    """
    scraper = None
    try:
        print(f"[API] Validating credentials for: {email}", file=sys.stderr)
        
        # Initialize scraper WITH session persistence so user doesn't need to log in again
        scraper = SRMAcademiaScraperSelenium(headless=True, use_session=True, user_email=email)
        
        # Attempt login (this will create/save the session)
        login_success = scraper.login(email, password)
        
        if login_success:
            print(f"[API] Credential validation successful for: {email}", file=sys.stderr)
            print(f"[API] Session created - future requests won't need password", file=sys.stderr)
            return {
                "success": True,
                "message": "Credentials validated successfully",
                "email": email,
                "session_created": True
            }
        else:
            print(f"[API] Credential validation failed for: {email}", file=sys.stderr)
            return {
                "success": False,
                "error": "Invalid credentials"
            }
        
    except Exception as e:
        print(f"[API] Error validating credentials: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return {
            "success": False,
            "error": f"Validation error: {str(e)}"
        }
    
    finally:
        if scraper:
            try:
                scraper.close()
                print("[API] Scraper closed", file=sys.stderr)
            except Exception as e:
                print(f"[API] Error closing scraper: {e}", file=sys.stderr)


# ============================================================================
# MAIN ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    try:
        # Read JSON input from stdin
        input_data = json.loads(sys.stdin.read())

        action = input_data.get('action')
        email = input_data.get('email')
        password = input_data.get('password')
        force_refresh = input_data.get('force_refresh', False)

        # ============================================================================
        # ACTION-AWARE VALIDATION
        # ============================================================================
        # Some actions require password (for initial authentication)
        # Some actions don't require password (they use existing sessions)
        # ============================================================================

        if action == 'validate_credentials':
            # Validate credentials: REQUIRES password
            if not email or not password:
                result = {"success": False, "error": "Email and password required"}
            else:
                result = api_validate_credentials(email, password)

        elif action == 'get_all_data':
            # Get all data: email REQUIRED, password OPTIONAL (uses session if available)
            if not email:
                result = {"success": False, "error": "Email is required"}
            else:
                result = api_get_all_data(email, password, force_refresh)

        elif action == 'get_static_data':
            # Get static data (Calendar + Timetable): email REQUIRED, password OPTIONAL (uses session if available)
            if not email:
                result = {"success": False, "error": "Email is required"}
            else:
                result = api_get_static_data(email, password, force_refresh)

        elif action == 'get_dynamic_data':
            # Get dynamic data (Attendance + Marks): email REQUIRED, password OPTIONAL (uses session if available)
            if not email:
                result = {"success": False, "error": "Email is required"}
            else:
                result = api_get_dynamic_data(email, password)

        elif action == 'get_calendar_data':
            # Calendar: REQUIRES password
            if not email or not password:
                result = {"success": False, "error": "Email and password required"}
            else:
                result = api_get_calendar_data(email, password, force_refresh)

        elif action == 'get_timetable_data':
            # Timetable: REQUIRES password
            if not email or not password:
                result = {"success": False, "error": "Email and password required"}
            else:
                result = api_get_timetable_data(email, password)

        elif action == 'get_attendance_data':
            # Attendance: REQUIRES password
            if not email or not password:
                result = {"success": False, "error": "Email and password required"}
            else:
                result = api_get_attendance_data(email, password)

        elif action == 'get_marks_data':
            # Marks: REQUIRES password
            if not email or not password:
                result = {"success": False, "error": "Email and password required"}
            else:
                result = api_get_marks_data(email, password)

        else:
            result = {"success": False, "error": f"Unknown action: {action}"}
        
        # Output result as JSON (only once)
        print(json.dumps(result))
        sys.exit(0)
        
    except Exception as e:
        print(json.dumps({"success": False, "error": str(e)}))
        sys.exit(1)

